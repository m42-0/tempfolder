# Favorite Visualizations
## Favorite Visualization: Amy Visualization of the displacement of 10 million enslaved Africans over the course (3+ centuries) of the Atlantic slave trade. 

slavevoyages.org  
## Julia “200 Years of Immigration to the US”  
## Isha 
https://shameplane.com/  
## Rutvik Image Source Network Graph of the relationships between the characters in HBO’s ‘The Wire’  
## Justin Benefits of Electric Cars  
## Krishi The number of moves it takes a knight to get around the chess board  
## Favorite Visualization: Jay Graphing the history of philosophy  
Image Source  
## Serdar  
Global GDP 2021  
## Aditya

## Data are Everywhere  
Data are Everywhere 
● Humanities: The complete works of William Shakespeare 
● Social sciences: sociology, political science, public health, economics, etc. 
● Natural sciences: physics, astronomy, oceanography, biology, neuroscience, etc. 
● Sports  

### Predict elections 

● Study demographics 
● Campaign managers study voters and target their messages accordingly  

### Politics  

### Healthcare  

### Diagnosis 

● Treatment plans 
● Epidemic watch  

### Industry Airlines 

● Price setting 
● Route planning 
● Revenue management 
● Frequent flyer program design  
Image Source  
Image Source  

### Observation by Michael Franklin (University of Chicago Computer Science Professor) 

● 1970’s: the confluence of electrical engineering and maths led to the birth of the field of Computer Science 
● 2010’s: the confluence of computer science and statistics, together with relevant domain knowledge, is prompting the growth of a new field called Data Science  

### The fourth scientific paradigm 
1. Theoretical 
2. Experimental 
3. Computational 
4. Data-driven: Empirical  

### Proceed with caution 
● Algorithmic Bias Q&A with Cynthia Dwork 
● When Discrimination is Baked into Algorithms 
● Fairness, Accountability, & Transparency Conference  

### Goals of Data Science  
Herb Simon: “Basic” vs. “Applied” Science 
● Basic science = Descriptive & Explanatory goals ○ To know: i.e., “to describe the world” ○ To understand: i.e., “to [explain] phenomena” 
● Applied science = Predictive goals ○ “Laws connecting sets of variables allow … predictions to be made from known values of some of the variables to unknown values of other variables.”  
What are the goals of data science? 
● Description: describing patterns in data ○ Descriptive statistics ■ Numerical summaries: tables ■ Visualizations (i.e., visual summaries): plots 
● Explanation: explaining patterns in data ○ Tell a causal story (e.g., smoking causes cancer) ○ Tell an effect story (e.g., the effect of smoking on health) 
● Prediction: predicting patterns in unseen data ○ Model potentially complex relationships in observed data, and use the model to make predictions about unobserved data  
What are the goals of data science? 
● Abductive Reasoning 
● Inductive reasoning 
● Deductive reasoning  
Data 
● We might have data about middle-age, middle-class women (like me!) living in Providence, RI 
● We might have a snapshot of these data, or the data set could be longitudinal (i.e., span multiple years) 
● If the data concern women from, say, the 1950’s, we might even have labels: e.g., cause of death  
Descriptive Goal of Data Science 
● We can summarize the data by calculating the average age of death, the most common cause of death, etc. 
● With longitudinal data, we can plot weight, height, etc., over time 
● Basic tools are descriptive statistics ○ Numerical summaries: tables ○ Visualizations (i.e., visual summaries): plots  
Explanatory Goal of Data Science 
● We learn a causal model that is intended to explains which features a woman possesses may cause her to die of cancer 
● Some tools come from machine learning and optimization: ○ Assume a machine learning model: e.g., a “true” functional form ○ Learn a function that minimizes error in predictions ○ Prioritize the model’s interpretability of over its accuracy 
● Other tools are statistical in nature: ○ Assume a statistical model: e.g., a “true” distributional form ○ Use data/observations to estimate the parameters of the model ○ Use the model to make causal inferences, where possible  
Predictive Goal of Data Science 
● We learn a model that predicts the likelihood that a woman characterized by a certain set of features will die of cancer 
● Some tools come from machine learning and optimization: ○ Assume a machine learning model: e.g., a “true” functional form ○ Learn a function that minimizes error in predictions ○ Prioritize accuracy. Function may be very complex. 
● Other tools are statistical in nature: ○ Assume a statistical model: e.g., a “true” distributional form ○ Use data/observations to estimate the parameters of the model ○ Use the model to make predictions about new data/observations  

### Methods of Data Science 
How do you do Data Science? (Colin Mallows) 
1. Identify data to collect and its relevance to your problem 
2. Statistical specification of the problem 
3. Method selection 
4. Method implementation 
5. Interpret result for non-statisticians  

How do you do Data Science? (Ben Fry) 
1. Acquire 
2. Parse 
3. Filter 
4. Mine 
5. Represent 
6. Refine 
7. Interact

How do you do Data Science? (Peter Huber) 
1. Inspection 
2. Error Checking 
3. Modification 
4. Comparison 
5. Modeling and model fitting 
6. Simulation 
7. What-if analyses 
8. Interpretation 
9. Presentation of conclusions

How do you do Data Science? (Galit Shmueli) 
1. Define goal 
2. Design study and collect data 
3. Prepare data 
4. Exploratory data analysis 
5. Choose variables
6. Choose methods 
7. Evaluate, validate, and model selection 
8. User model and report Source

How do you do Data Science? (CSCI 0100) 
1. Define goal 
2. Find and prepare data 
3. Exploratory data analysis 
4. Choose variables and methods (i.e., build models) 
5. Evaluate, validate, and model selection 
6. Report (explanations or predictions)

1. Descriptive Statistics: Summarizing Data ○ No underlying model, statistical or otherwise ○ No machine learning, statistical estimation, or statistical inference ○ Just Exploratory Data Analysis Examples ○ Histograms, conditional histograms ○ Measures of central tendency ○ Measures of dispersion  
2. Classic Statistics ○ Law of Large Numbers ○ Central Limit Theorem ○ Confidence Intervals ○ Hypothesis Testing Example Applications ○ Analyzing clinical trials to predict drug efficacy ○ Analyzing polling data to predict election outcomes  
3. Classic Machine Learning ○ Assume a functional form ○ Learning, so training on in-sample data ○ Prediction: Inductive, out-of-sample forecasting Example Methods ○ Decision and regression trees ○ k-nearest neighbors

### Statistical Machine Learning (i.e., Estimation and Inference) 

○ Assume an underlying statistical model of a population 
■ Selects a few key variables of interest 
■ Might describe how they relate to one another 
■ Might make assumptions about how they are distributed 

○ Estimate the parameters of the model, using in-sample data 
■ Example estimators: sample mean, sample variance, etc. 
■ Example techniques: maximum likelihood, maximum a posteriori, etc. 

○ Inference: Apply the model to generalize to out-of-sample data  

Model desiderata 
○ Plausible 
○ Interpretable 
○ Simple (“the simplest explanation is best”) 
○ Generalizable (i.e., still relevant, beyond any sample) 
Model checking is key! “All models are wrong, but some are useful.” -- George Box  
Data cleaning (yuk!) 
● Data visualization (fun!) 
● Structured, as well as unstructured, data 
○ Text, maps, social networks, etc. 
● Algorithm bias, data privacy and provenance, etc.  
[Seeing Theory (brown.edu)](https://seeing-theory.brown.edu/)  

○ The Cartoon Guide to Statistics ○ Naked Statistics, by Charles Wheelan  

## Big Data
“Extremely large data sets that may be analyzed computationally to reveal patterns, trends, and associations.”  
Data Mining Extracting comprehensible information from data  
Data Munging/Wrangling/Jujitsu Converting data from one "raw" form into another form, which is often cleaner and more structured  
Predictive Modeling Building a statistical model of unknown behavior Predictive Analytics Making predictions about unknown future event

http://hint.fm/
https://www.ted.com/talks/david_mccandless_the_beauty_of_data_visualization?language=en
https://www.ted.com/talks/talithia_williams_own_your_body_s_data?language=en#t-1009468

# Two Historic Case Studies  
## John Snow: Data Scientist  
London in the 1850’s  
London in the 1850’s was wealthy, but many citizens lived in extreme poverty ● Disease was rampant, especially cholera ● Causality between unsanitary food and water and diseases was not yet accepted ● Miasma theory: bad air was generally believed to be the cause of cholera  
Cholera in London in the mid 1800’s ● The first cases of cholera in London were reported in 1831 ● Cholera killed hundreds of Londoners in one week in 1854 ○“Within 250 yards of the spot where Cambridge Street joins Broad Street, there were upwards of 500 fatal attacks of cholera in 10 days,” Dr. Snow wrote. ● Patients died within a day or two of contracting the disease ● One wave of cholera could kill tens of thousands  
John Snow ● John Snow, an obstetrician/anaesthesiologist, was skeptical of the miasma theory. If the disease was airborne, wouldn’t people living in the same area be breathing in the same toxins? ● But entire households were dying of cholera, while their neighbors were perfectly fine.  
Snow’s Observations ● Symptoms included vomiting and diarrhea ● This evidence led Snow to believe the disease was food/drink borne as opposed to airborne ● He hypothesized that some drinking water was contaminated  
Plotting Cholera ● August, 1854 ● Mapped out deaths ● Black bars show deaths ● Black circles (inside red circles) show water pumps  
(image drawn and lithographed by Charles Cheffins)  
Snow’s Hypothesis ● Snow observed that deaths were clustered around Broad Street Pump ● Other Evidence ○ Lion Brewery ○ Rupert Street Pump ○ Scattered houses, school kids, prison (image drawn and lithographed by Charles Cheffins)  
Further Evidence ● Two women died of Cholera in Hampstead which was not near Soho ● Snow learned that the two women used to live on Broad Street ● They liked the taste of the water from the Broad Street pump so they had it delivered to them everyday  
Could Snow prove that water from the Broad Street Pump was causing cholera? ● Snow suspected that the water supply was a clue to understanding cholera outbreaks ● But he could not prove this hypothesis ● He applied a statistical method, called the method of comparison ● But to be clear, statistics can never be used to prove anything; they can only be used to argue that an outcome is unlikely  
Individuals comprise the two test groups ○ Treatment ○ Control ● Treatment: a process that the “treatment” group undergoes ● Control: a group similar to the treatment group in every way, except that it does not undergo the treatment ● Outcome: the results observed in the two groups  
Method of Comparison  
Method of Comparison  
Question: does the treatment have an effect on the outcome? ● If the outcome between the treatment and control differs significantly, we can conclude that there is an association. ● But does the treatment cause the outcome to occur?  
Confounding Factors  
Confounding factor: an underlying difference between the treatment and control groups that is not the treatment ● TVs and SAT scores ○ More TVs in students’ homes correlates with higher SAT scores ○ Does watching more TV cause students to do well on the SATs? ● Coffee and Lung Cancer ○ People who have lung cancer also tend to drink coffee ○ Does drinking coffee cause lung cancer?  
Snow’s “Grand Experiment” ● Lambeth water company vs. Southwark and Vauxhall (S&V) ○ S&V homes were the treatment group ○ Lambeth houses were the control group ● Snow argued there were no confounding factors, because there were no substantive differences between the populations to whom the two companies provided water “Each company supplies both rich and poor, both large houses and small; there is no difference either in the condition or occupation of the persons receiving the water of the different Companies … there is no difference whatever in the houses or the people receiving the supply of the two Water Companies, or in any of the physical conditions with which they are surrounded …”  
Data collected by John Snow  
Plotting Deaths / Household Data  
Causality Snow observed that the only difference between the area where people were getting sick and the area where they weren’t was that “one group being supplied with water containing the sewage of London, and amongst it, whatever might have come from the cholera patients, the other group having water quite free from impurity.” Snow convinced the authorities to remove the handle of the Broad Street pump. It was later determined that there was a leaking cesspool a few feet away from the Broad Street pump well, and sewage from the households of cholera victims was seeping into the well water  
Success! In 1866 there was another outbreak of Cholera in the Limehouse district of London.  
A Little More History ● In 1854 (at about the same time Snow was drawing these maps in London), an Italian scientist named Filippo Pacini discovered a bacterium called Vibrio cholerae that enters the small intestine and causes cholera ● But because of the popularity of the miasma theory, Pacini’s discovery received little to no immediate attention ● In 1883, a German scientist named Robert Koch discovered Vibrio cholerae again, and the root cause of cholera was finally uncovered  
An Early Example of the Power of Data Visualization ● Snow’s data collection and data visualization saved many lives ● Snow pioneered the use of data visualization to map disease ● Nowadays, it is standard to use disease maps to track epidemics ● To this day, scientists at the Center for Disease Control (CDC) still use the phrase “Where is the handle to this pump?” when trying to uncover the cause of an epidemic  
## Florence Nightingale: Data Scientist  
Nurse or Data Scientist? While Florence Nightingale (1820-1910) is best known as the founder of modern nursing and worldwide health-reform, in fact, she was at heart a statistician, and a pioneer in data visualization.  
The Crimean War ● Nightingale worked as a nurse during the Crimean War. ● She found the hospital to be highly unsanitary. ● Many soldiers were dying, but why? ● From battle wounds, or other causes, such as poor hygiene? ● Nightingale decided to collect data in attempt to settle this question. ● Her approach worked. By the time Nightingale left Crimea, the mortality rate in hospitals had dropped from 42% to 2%.  
The Power of Visualization  
Afraid Queen Victoria and Parliament would not read or understand her statistical report, Florence Nightingale created a graphical representation to convey her findings.  
Polar Area Graph (Coxcomb)  
● Each wedge represents a month from April 1854 to March 1856 ● Blue: death by disease ● Red: death by wounds ● Black: death by other causes  
Image Source  
Saving Lives ● Deaths are greater from April 1854 to March 1855 than the following year ● Deaths by disease are much greater ● A sanitary commission arrived from Britain in March 1855 to improve the sanitation of the hospitals  
Image Source  
Map of a later cholera outbreak in London, in 1866  
Reproduction of Nightingale’s Graph  
The Statistical Inference Process, via Example ● Start by choosing a population to study, and some features of that population: e.g., citizens of London in the 1850’s ● Establish a baseline for that population: what is the probability of someone living in London then contracting cholera? ● Sample the population: Snow studied two samples ○ One, the control, provided a baseline distribution (.37% chance) ○ The treatment provided a second distribution (3.15% chance) ● Snow argued that it was extremely unlikely that the treatment sample was drawn from the same distribution/population as the control ● Statistics can never be used to PROVE anything; but they can be used to argue that an outcome is likely or unlikely  
Population ● A population is a set of individuals under study, chosen by the statistician, or nowadays, the data scientist ● When the entirety of the population is observable, we can compute descriptive statistics like population mean, population variance, histograms, etc. ● But typically, the entirety of the population is unobservable  
Sample ● A sample is an observed subset of the population ● A statistician’s goal is to study the sample, and from it, draw inferences about the entirety of the population ● Polling is a typical application of statistical inference: ○ E.g., ask a few Brown students about their favorite anything; then infer the favorite anything of most Brown students  
[www.ph.ucla.edu/epi/snow/snowcricketarticle.html](http://www.ph.ucla.edu/epi/snow/snowcricketarticle.html)  
[Barnyard Dust Offers a Clue to Stopping Asthma in Children - The New York Times (nytimes.com)](https://www.nytimes.com/2016/08/04/health/dust-asthma-children.html?_r=0)


# Three Modern Case Studies  
## Moneyball: A Sports Case Study  
Setting the Scene ● The Athletics (A’s) are a baseball team that plays in Oakland, CA. ● In the early 2000s, the A’s were a low budget baseball team. ● The owners of the team refused to increase their budget. ● The A’s lost their 3 best players to teams that were willing to pay more. ● Since there is no salary cap in baseball, the A’s had to find a way to remain competitive with much wealthier teams.  
Baseball Management meets Data Science  
Billy Beane was the general manager of the A’s at that time. ● Beane began to look for undervalued players who he thought could become stars in the league. He took the revolutionary step of using data to help decide how to formulate his roster. ● Before Beane, baseball scouts decided which players were destined for stardom after observing a few games, and their gut feelings. ○ Scouts would judge appearances of players, and even their spouses! ○ Using his data-driven approach, Beane often came up with conclusions that contradicted what his top scouts thought. ○ Beane was derided by professional scouts and the media.  
The 2002 Athletics Season  
The A’s began the season with 20 wins and 26 losses. Deriders of the “Moneyball” strategy gloated, contending a data-driven method could never beat the work of professional scouts. ● The A’s, however, recovered by winning 16 out of 17 games in June. Between August and September, the A’s won 20 games in a row, setting the American League record at that time (2016). ● Overall, the A’s finished with 103 wins, tied with the Yankees for the most in the major league. ○ The A’s had the 3rd smallest payroll in the league, at less than $40Mn ○ The Yankees had the largest payroll, at over $125Mn  
How Did the A’s Do It?  
At the core of the A’s success was Sabermetrics (SABR is the Society for American Baseball Research): i.e., using data to predict baseball-player performance. Fielding: ● More athletic players have better range on the field. This enables these players to get to more baseballs. ● Being able to get to more balls (especially if you get to them in a difficult spot) leads to more errors. ● Players with a lot of errors were seen as poor fielders by scouts, even though they’d reach balls that others couldn’t reach. ● Beane innovated here, using advanced statistics such as “defensive runs saved”, to measure fielding ability. Batting: ● Consider the RBI (runs batted in): this statistic is meant to capture how many runs a batter generates, but ignores the batter himself! ● For example, if a batter hits a triple with no one on base, the batter has still put the team in a good position to score. ● Beane created and used other novel statistics such as “on-base percentage” and “runs created” to more accurately capture a batter’s ability to generate runs.  
The success of the Oakland A’s provided the first really compelling evidence that a data-driven method could work in sports. ○ The Moneyball strategy has been adopted by other sports. ○ The Rockets of the NBA and the Blackhawks of the NHL provide evidence this approach can work throughout sports. ● Statistics cannot guarantee a playoffs win, but can get a team to the playoffs! ○ Moneyball requires large sample sizes; over the 162 game baseball season, a player’s statistics will balance out to reveal his true value. ○ However, playoff series are much shorter (either 1, 5, or 7 games), so invariably, other factors, including luck, come into play.  
Tennis  
In tennis, an unforced error occurs when a player hits the ball either into the net or out of bounds. ● While it may appear that having a high number of unforced errors is bad, players make unforced errors when they go for tough shots. ● So players with a high rate of unforced errors could actually be going for more shots, so they could likewise be hitting more winners. ● A more useful, but less cited, statistic is the ratio of winners to unforced errors; indeed, the player with the higher ratio wins about 90% of the time.  
Hockey / Basketball  
In hockey and basketball, turnovers (when the ball changes hands from one team to the other) are tracked. ● Common sense would dictate: giveaways are bad and takeaways are good. ● However, players can only record takeaways when their team does not have the ball, and can only record giveaways when their team does have the ball. ● Good teams have the ball more, giving the players more opportunities for giveaways rather than takeaways. ● In the 2015-16 NBA season, 7 of the 10 leaders in giveaways were recent all-stars; they had the ball more, and in turn, they gave it away more. ● Giveaways turns out to be a misleading statistic.  
Conclusion  
In Moneyball, Beane created statistics that more accurately measured what traditional statistics were trying to capture. ● In the other sports, data analysts followed suit, discrediting statistics which sports commentators and fans had come to believe in. ● Because shortcomings of the “eye-test” (watching and judging games without statistics), data science, specifically predictive analytics, is gaining acceptance in sports.  
## A Politics Case Study  
Founded by Nate Silver, who first won acclaim for his work in Sabermetrics ● Correctly predicted the outcome in every state in the 2012 presidential election ● Correctly predicted every Senate election outcome in 2008  
Goal (in a presidential election) Predict a probability that the Democrat wins, a probability that the Republican wins, and perhaps a probability that a third or fourth party candidate wins, as well. Intermediate Goal Build a probability distribution over all possible electoral vote outcomes (e.g., 332 vs. 206; 173 vs. 365; etc.). Then tally the probabilities associated with the various wins.  
What if the election were today? Inter-state poll aggregation: 1. Using the results of today’s per-state polls, compute a probability distribution over all possible electoral vote (i.e., national) outcomes 2. Aggregate over electoral vote outcomes to compute each party’s win probability  
Who will win some state?  
Underlying assumption Per-state polls taken on election day correctly predict national election outcomes Success of Gallup Polls  
Further Intermediate Goal  
Use past and current poll data to predict the results of election-day polls  
How can we predict future polls from past?  
Answer ● (Statistical) machine learning ● Specifically, regression, to infer a trend line  
Knowing that the election is not today...  
Inter-state poll aggregation: 1. Predict future (election-day) polls from past polls 2. Using the predicted future per-state polls, compute a probability distribution over all possible electoral vote (i.e., national) outcomes 3. Aggregate over electoral vote outcomes to compute each party’s win probability  
Aggregating intra-state polls  
Calculate the center of all the polls (mean, median, etc.) Calculate the spread among the polls: ● Professor Wang’s (Princeton) approach: how much do the polls differ from one another? ● 538 approach: how accurate have these pollsters been in the past? ● Correct approach: how do the polls vary from how the population would have voted had the actual election been the day of the poll?  
Competing approaches  
Professor Wang: assumes independence across state polls, and calculates the probability distribution in closed-form ● 538 calculates correlations between states (e.g., outcomes in OH and PA might be highly correlated), but is then forced to resort to simulation to estimate the probability distribution  
538 Modeling Principles Model Desiderata 1. Probabilistic, not deterministic 2. Empirically sound (empirically justifiable assumptions) 3. Not unstable: it should not oscillate wildly given additional inputs 4. Do not tweak if the prediction is undesirable  
What are the best predictors? ● Fundamentals at the national, regional, or state level ○ Demographic makeup (race, religion, etc.) ○ Economic indices ● Per-state polls  
Polls-only vs. polls-plus models  
How can we predict future polls from past? Answer ● (Statistical) machine learning ● Specifically, regression, to infer a trend line Technical detail: ● Predict per-poll, and then aggregate to produce per-state probability distributions ● Or simplify, by aggregating state polls first, and then predicting a single per-state probability distribution  
## Netflix: An Internet Case Study  
Netflix is data-driven ● A typical subscriber will only look at about 10 to 20 titles, and only a couple of rows of recommendations ● If the user does not find something of interest in the first 90 seconds, Netflix runs the risk of them abandoning the service ● Netflix has spent more than a decade and lots and lots of money refining its recommendation system ● 75 percent of viewer activity is driven by other users’ recommendations ● Netflix wants its service to be so engaging that users pick it over other end-of-the-day activities, like reading a book or magazine, TV or Facebook  
Collaborative filtering ● Method of making predictions (filtering) about the interest of one user based on the tastes and preferences of other users (collaborative) Underlying assumption ● If A has the same opinion as B on an issue (e.g., liking or disliking a movie), then on some other issue, it is more likely that A will have the same opinion as B than the same opinion as some other person chosen at random  
Netflix Prize ● A competition for the best collaborative filtering algorithm ● Goal: to predict user ratings for films based on previous user ratings, without any additional information about the films or the users ● The data consisted of 100,480,507 ratings by 480,189 users of 17,770 movies ● On 21 September 2009, BellKor's Pragmatic Chaos team won the grand prize of US $1,000,000, besting Netflix’s own algorithm by 10.06%  
Exploration vs. Exploitation ● Netflix is careful not to over-personalize ● Chris Jaffe, Netflix’s vice president of product innovation, loves dark TV dramas ● The algorithm knows this, so it usually points him to shows of that genre (exploitation), but every now and then it throws in something else (exploration), like a horror film, to gauge Jaffe’s interest ● Netflix risks upsetting Jaffe by doing so, but learns more about Jaffe’s interests  
Experimental design at Netflix ● Netflix A/B tests everything from images to the size of font on the screen  
Experimental design at Netflix  
Netflix A/B tests everything from images to the size of font on the screen ● Netflix tested whether images that appear at the top of the screen should be static or a carousel ○ A rotation of three images worked best ● Netflix tested whether detailed synopsis make users more likely to watch a title ○ It turned out that a couple of short and clear sentences is best ● Netflix uses a large set of users (roughly 300,000, worldwide) in its A/B tests to increase its confidence that its design changes will indeed increase engagement

[Netflix Prize - Wikipedia](https://en.wikipedia.org/wiki/Netflix_Prize)

# Databases vs. Spreadsheets  
Raw data can be unmanageable

What company appears most often in the data set? ○ Walmart ● What nationality is most common? ○ U.S. ● Who is the youngest person? ○ Mark Zuckerberg ● What is the average networth of the people in the dataset? ○ $48.41 Billion  
Raw data can be unmanageable ● It is often impossible to answer these or similar queries, or to establish any trends, by eyeballing the data. ● Computational processing is necessary for data beyond a trivial size.  
Databases ● A database is a structured set of data that are easily accessible in various ways ● Database software tools facilitate the automated management of data ○ Storing, searching, modifying, and extracting information in a database ● Database systems can manage a very large quantity of data  
Database Software ● Database Management Systems: software that handles storage, retrieval, and updating of databases ○ Examples: Oracle, MySQL, Access ● Statistics Packages ○ Examples: SAS, SPSS, Stata, R ● Spreadsheets: they do a little of both, at human scale, and for human comprehension, with minimal human effort!  
Spreadsheets ● Suitable for managing relatively small databases ● Designed to facilitate human comprehension of data ● Useful for visualizing both raw data, in tabular form, and summaries of data in the form of charts, etc. ● Data management is usually accomplished manually, not via automated tools  
History of Databases  
Public School Database then ● Before computers were commonplace, people used other systems to track student data ● Schools often used paper-based books (can you imagine?!) for grades, attendance, student contact info, etc. ● These books would be divided into many small squares where teachers would record relevant information  
Difficulties with paper-based records  
Mistakes are hard to correct and quickly become quite messy ● It is difficult to search through the whole book (slowly or quickly) ● Calculating summary statistics is difficult (e.g., how many days did R.N. Downing miss school?) ● Records can be lost forever or damaged due to water, fire, etc., because it is expensive to store backups  
Public School Database now ● Make heavy use of (hopefully, secure) spreadsheets ● Mistakes are easy to fix (and make!) in spreadsheets ○ One can simply overwrite an entry ● Can easily search for specific entries ○ E.g., did R.N. Downing attend school on 10/14? ● Spreadsheets can also be used to easily calculate summary statistics, like grade-point averages and total number of days absent ● Records are backed up on some online system, so they are less likely to be damaged like paper books  
History of Spreadsheets  
History of Spreadsheets ● The word spreadsheet comes from the word “spread,” as in a newspaper or magazine ● “Spreadsheet” was used to refer to bookkeeping ledgers with all revenue, costs, taxes, etc. spread on a single sheet of paper (or across two pages of a bound ledger) ● In 1961, Professor Richard Mattessich pioneered computerized spreadsheets for business accounting on main frames: "Spreadsheet: Its First Computerization (1961-1964)"  
VisiCalc: Dan Bricklin & Bob Frankston  
VisiCalc Big idea: Instant automatic recalculation based on formulas stored in the cells referencing other cells  
Lotus 1-2-3 ● January 1983 ● Mitch Kapor and Jonathan Sachs ● More powerful than VisiCalc ● Very popular ● Lotus 1-2-3 is thought to be one of the reasons the IBM PC was so successful. Likewise, for VisiCalc and the Apple II.  
Microsoft Excel ● Mid 1980’s ● First spreadsheet to use a GUI, or graphical user interface, meaning not an entirely text-based interface ● Was a big selling point for Apple’s Macintosh; people bought it because it came with Excel ● In 1987, Microsoft released Windows with Excel  
Today there are many options ● Microsoft Excel ● Google Spreadsheets ● LibreOffice Calc  
Data can be either qualitative or quantitative  
Qualitative data can be nominal or ordinal  
Nominal means that there is no natural order among the values  
Ordinal means that there is a natural ordering  
Quantitative data can be either discrete or continuou  
[The richest people in the world: billionaires across the globe (cbsnews.com)](https://www.cbsnews.com/pictures/richest-people-in-world-forbes/21/)  
[Netflix Prize - Wikipedia](https://en.wikipedia.org/wiki/Netflix_Prize)  
[Free ICT Resources - new content for IGCSE 2016 syllabus coming (ictlounge.com)](https://www.ictlounge.com/)  
[VisiCalc - Wikipedia](https://en.wikipedia.org/wiki/VisiCalc)  
[qual_quant.jpg (490×278) (google.com)](https://sites.google.com/a/staff.lisd.net/mr-nic-s---8th-grade-pre-ap-science/_/rsrc/1378704557909/unit-1---forces-motion/qualitative-data-vs-quantitative-data/qual_quant.jpg)  
[nominal_level.jpg (338×151) (restore.ac.uk)](https://www.restore.ac.uk/srme/www/fac/soc/wie/research-new/srme/modules/mod1/3/nominal_level.jpg)  
[hotscale.jpg (332×229) (statisticshowto.com)](https://www.statisticshowto.com/wp-content/uploads/2013/09/hotscale.jpg)  
[Attendance.ods (live.com)](https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fcs.brown.edu%2Fcourses%2Fcs100%2Flectures%2Fscripts%2Flecture2a%2FAttendance.ods&wdOrigin=BROWSELINK)  
Introduction to Spreadsheets  
What are formulas? ● Formulas are used to perform calculations on data in a spreadsheet ● Formulas always start with = ● Formulas consist of a function, like + or -, and arguments ● Arguments are what the function is applied to: either values (e.g., numbers) or references (defined on the next slide) ● An example of a simple formula is =1+2  
What are references? ● A reference is a cell address, like A1 or A2 or B2 and so on ● An example of a formula involving references is =A1-B1 ● If the value in A1 is 100, and the value in cell A2 is 1, then this formula evaluates to 99  
More about formulas  
+, -, *, / are called binary functions, because they apply to two numbers! ● - can also be a unary function, when it is the negative operator applied to just one number ● But some functions can be applied to an arbitrary number of arguments, like: sum, average, min, max, etc.  
The syntax for formulas involving these functions is:  
Like other formulas, they always start with = ● = is followed by a function name, and parentheses ● Inside the parentheses is a data range, such as A1:A5  
Example dataset: Revenue and Expenses of (a few) Ivy League Schools ● From financial year 2016 ● Money is in millions of US dollars ● Source is from various schools’ financial reports  
First, let’s compute Brown’s total revenue ● In column H create a new column and label it Total Revenue ● Click on cell H2, the first cell in this new column ● In the Formula bar type = ● Then click on the cell in row 2 of the column labeled money Endowment Support, type +, and then click on the cell in row 2 of the column labeled money Student Fees ● Hit enter and observe the total amount of money raised by Brown  
Next, let’s compute all these schools’ total revenues ● Click on the lower right corner of H2, and drag your mouse down to populate the next 3 cells. ● Click on any of those 3 cells, and observe the formula that appears in the formula bar for that cell. ● Spreadsheets are very “smart”. By default, they assume relative references: they adjust formulas as necessary to reference the relative row or column. ● Absolute references are specified using dollar signs: E.g., ○ $A1 to make the roq absolute ○ A$1 to make the column absolute ○ $A$1 to make both the row and the column absolute  
Total Revenue  
Who had the highest total revenue?  
Click on the cell at the bottom of the Total Revenue column ● In the Formula bar write =MAX( ● Then click on first cell under Total Revenue, type :, and then click on the bottom cell under Total Revenue ● Then end the formula with ), and hit Enter  
Who had the lowest total revenue?  
Click on the cell at the bottom of the Total Revenue column ● In the Formula bar write =MIN( ● Then click on first cell under Total Revenue, type :, and then click on the bottom cell under Total Revenue ● Then end the formula with ), and hit Enter  
Now, let’s compute Brown’s total expenses  
● In column I create a new column and label it Total Expenses ● Click on cell I2, the first cell in this new column ● In the Formula bar type = ● Then click on the cell in row 2 of the column labeled Student Aid, type +, and then click on the cell in row 2 of the column labeled Operating Expense ● Hit enter and observe the total amount of money spent by Brown  
Next, let’s compute all these schools’ total expenses ● Click on the lower right corner of I2, and drag your mouse down to populate the next 3 cells. ● Click on any of those 3 cells, and observe the formula that appears in the formula bar for that cell. ● Spreadsheets are very “smart”. They adjust formulas as necessary to reference the appropriate row.  
Total Expenses  
Who had the highest total expenses? ● Click on the cell at the bottom of the Total Expenses column ● In the Formula bar write =MAX( ● Then click on first cell under Total Expenses, type :, and then click on the bottom cell under Total Expenses ● Then end the formula with ), and hit Enter  
How about finding the average amount of money spent? ● Click on the cell at the bottom of the Total Expenses column ● In the Formula bar write =AVERAGE( ● Then click on the top cell under Total Expenses, type :, and then click on the bottom cell under Total Expenses ● End the formula with ), and hit Enter  
Now, let’s subtract total expenses from total revenue to determine total profits ● Create a new column (in column J) and label it Total Profit ● Click on the first cell in this new column, J2 ● In the Formula bar type = ● Then click on H2; then enter -; then click on I2 ● Click on the lower right-hand corner of the cell you just populated, and drag it down to populate the 3 cells below it ● Click on any of those 3 cells, and observe the formula that appears in the formula bar for that cell  
Who had the lowest profits?  
Some formulas manipulate text ● Len: returns the length of a string ● Concatenate: combines two strings into one ● Trim: removes duplicate spaces, and spaces at the start and end of a string ● Exact: tests if two supplied text strings are exactly the same and if so, returns true, else it returns false. (This function is case sensitive.)  
Let’s see an example of concatenate ● Click on column B ● From the drop-down menus: Insert -> Column left ● Name this new column President ● Click on cell B2 ● In the Formula bar, write =CONCATENATE(C2, " ", D2) ● As usual, drag the contents of B2 down to also populate the 3 cells below it  
Summary ● Spreadsheets are incredibly powerful tools, and easy to use! ● Using formulas, we can manipulate data to compute summary statistics ○ MIN, MAX, AVERAGE, etc. ● We can also visualize data in spreadsheets, so we can more easily identify patterns, and uncover stories that are hidden in our data. ● However, we will also discover that they become unwieldy as our analyses become more complex  
Qualitative vs. Quantitative Data  
Qualitative data Qualitative data describe qualities, like color, texture, smell, taste, appearance, etc. Many qualitative data are categorical: e.g., ● the color of a ball (yellow, blue, or red) ● the brand of a product purchased (brand A, B, or C) ● whether a person is employed (yes or no)  
Quantitative data Quantitative data take on numerical values, so are typically ordinal Examples: ● age, weight, height, income, etc. ● the value of a country’s exports ● a batter’s number of home runs  
Quantitative data can be either discrete or continuous  
Data are discrete if the measurements are necessarily integral (i.e., integers) ● Data are continuous if the measurements can take on any value, usually within some range  
Likert Scale  
A range of satisfaction scores ● Used to measure a range of attitudes (not just the binary) ● A way to to convert qualitative data to quantitative  
[Five better indicators than GDP | World Economic Forum (weforum.org)](https://www.weforum.org/agenda/2016/04/five-measures-of-growth-that-are-better-than-gdp/)  
Measures of Central Tendency  
Mean, Median, and Mode  
Population study ● The population under study is the 65 students in the 2017 class. ● We asked how many languages they speak (besides English). ● Here were their responses: 0 1 0 0 1 1 0 0 1 1 2 1 4 1 2 1 2 2 1 2 0 1 1 2 1 0 2 0 1 0 0 1 1 2 0 1 1 1 1 0 2 1 1 1 0 1 1 2 0 1 2 0 1 0 1 1 1 1 1 1 1 0 2 0 1 ● Each response is a measurement, or an outcome ● In this lecture, we will discuss ways to summarize data from a sample, using these responses as an example  
Frequency distribution ● A count of the # of times each outcome occurs is called a frequency distribution. ● This information can be conveyed in a table or a plot. ● A plot of a frequency distribution of numerical data is called a histogram.  
Measures of Central Tendency Mean, Median, and Mode Population study ● The population under study is the 65 students in the 2017 class. ● We asked how many languages they speak (besides English). ● Here were their responses: 0 1 0 0 1 1 0 0 1 1 2 1 4 1 2 1 2 2 1 2 0 1 1 2 1 0 2 0 1 0 0 1 1 2 0 1 1 1 1 0 2 1 1 1 0 1 1 2 0 1 2 0 1 0 1 1 1 1 1 1 1 0 2 0 1 ● Each response is a measurement, or an outcome ● In this lecture, we will discuss ways to summarize data from a sample, using these responses as an example Frequency distribution ● A count of the # of times each outcome occurs is called a frequency distribution. ● This information can be conveyed in a table or a plot. ● A plot of a frequency distribution of numerical data is called a histogram. The mean of a frequency distribution ● The 2017 dataset 0 1 0 0 1 1 0 0 1 1 2 1 4 1 2 1 2 2 1 2 0 1 1 2 1 0 2 0 1 0 0 1 1 2 0 1 1 1 1 0 2 1 1 1 0 1 1 2 0 1 2 0 1 0 1 1 1 1 1 1 1 0 2 0 1 ● The mean (sum/sample size) of these numbers is 0.95 ● Note that no student reports knowing 0.95 languages ● Still, this number is one way of summarizing the frequency distribution: “On average, the students in this class know 0.95 languages (beyond English).” Another way to calculate the mean ● Use the frequency distribution ● Calculate the sum of the data points by first multiplying the outcome and frequency columns together, and then adding up the results sum = (0 x 18) + (1 x 34) + (2 x 12) + (4 x 1) = 0 + 34 + 24 + 4 = 62 ● The mean is then the sum divided by the sample size (65) mean = sum/sample size = 62/65 ∽ 0.95 Yet another way to calculate the mean ● We could also calculate the mean by multiplying the outcome column by relative frequencies (frequency/sample size), and adding up the results mean = (0 x 0.28) + (1 x 0.52) + (2 x 0.18) + (3 x 0) + (4 x 0.02) ∽ 0.95 ● The mean of a sample is the weighted average of the distinct outcomes ● The weights are the relative frequencies with which those outcomes occur Relative Frequencies: 18/65 ∽ 0.28 34/65 ∽ 0.52 ... But the mean is not perfect (No descriptive statistic ever is) Example: ● A ten person first-year seminar has 9 first-year students who are 18 years old, and one who is 45 ● The mean student age is 20.7 ● Yet almost no one in the class is even 20! ● The mean is not an ideal way to summarize the data in this case ● The median is the “middle” of a frequency distribution of ordinal data ● Assume a sample: 12, 5, 3, 4, 5 ● Sorting these data gives: 3, 4, 5, 5, 12 ● The median value of this sample is 5 ● If the sample size is even: e.g., 2, 3, 4, 5, 5, 12 ● The median is the mean of the middle two numbers ● (4 + 5)/2 = 4.5 ● Q: What is the median age in the aforementioned first-year seminar? The median of a frequency distribution The median is the “middle” point of a distribution ● Consider another sample: 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4 ● The median is the outcome that divides the frequency distribution of outcomes in half ● 50% of the outcomes are at or below the median, and 50% are at or above it ● The area in a histogram to the right of the median equals the area to its left Blue Area = Red Area 6 x 1 + 8 x 0.5 = 8 x 0.5 + 3 x 1 + 3 x 1 10 = 10 The mean is the “balance” point of a distribution ● You can think of the mean as the a distribution’s center of gravity ● The sum total of the distances of all the points less than the mean equals the sum total of the distances of all the points greater 2.15 (2.15-1) x 6 + (2.15-2) x 8 = (3-2.15) x 3 + (4-2.15) x 3 (1.15) x 6 + (.15) x 8 = (.85) x 3 + (1.85) x 3 8.1 = 8.1 iClicker Q: The mean vs. the median If a student scores above the median on an exam (in the top half of the class), did the student score above the mean? A: Yes, the student scored above the mean B: No, the student scored below the mean C: Not sure; there isn’t enough information to answer this question D: Not sure, because I don’t understand the mean and median well enough yet to even hazard a guess (but I’m interested in learning this stuff!) C (or D): Not Enough Information ● The student is not necessarily above (or below) the mean ● The mean is the “balance” point of a distribution ● The median is the “middle” point of a distribution ● The order of these two points can vary with the distribution Symmetric (i.e., non-skewed) distributions ● In a symmetric distribution, the left mirrors the right ● So the mean (the balance point) equals the median (the middle) ● E.g., the mean and the median of the frequency distribution for the sample 1, 2, 2, 3 are both 2 mean = median = 2 Asymmetric (i.e., skewed) distributions ● In an asymmetric distribution, the left does not mirror the right ● So the mean and the median are not necessarily equal ● E.g., the mean and the median of the frequency distribution for the sample 1, 2, 2, 10 are not equal median = 2 mean = 3.75 Mean vs. Median ● The gold sample has an outlier: 10 ● So its histogram is skewed right ● Likewise, its balance point (the mean) falls to the right of the middle point (the median) ● Ultimately, only 25% of the outcomes are above the mean median = 2 mean = 3.75 mean = median = 2 iClicker Q: The mean vs. the median If a student scores above the median on an exam (in the top half of the class), did the student score above the mean? A: Yes, the student scored above the mean B: No, the student scored below the mean C: Not sure; there isn’t enough information to answer this question D: Not sure, because I don’t understand the mean and median well enough yet to even hazard a guess (but I’m interested in learning this stuff!) ● A student who scores above the median might have scored 10, but they also might have scored 3 ● It’s impossible to tell if they scored above the mean or not! C: Not Enough Information median = 2 mean = 3.75 mean = median = 2 A real-world example of a skewed distribution ● Delay times are in minutes ● A negative observation means the flight left earlier than scheduled ● The histogram is right skewed ● It has a long right-hand tail ● The mean is being pulled away from the median in the direction of the tail, so we expect the mean to be greater than the median: mean = 9.135913 median = -2 Histogram of American Airlines Flight Delays Image source Another example of a skewed real-world distribution: Public school funding Mean vs. median, rule-of-thumb Image Source Bush Tax Cuts (2001/3) ● GOP reported that 92 million Americans would get a tax cut, averaging $1,083 ● But actually, the median was less than $100 ● Incredibly rich outliers received much larger tax cuts and skewed the mean Bernie Sanders’ Talking Points (2016) ● Average contribution is $27 ● Distribution should have been right-skewed (because it is lower-bounded by 0) ● So isn’t the median smaller still? ● I’m guessing it wasn’t Rule of Thumb ● For skewed distributions, the mean lies in the direction of the skew (towards the longer tail) relative to the median ● But this rule of thumb is not always true! Image Source Mode ● The mode is the most popular outcome; the one that occurs most often. ● The mode is useful for summarizing categorical data. In the histogram of letters in English text, the mode is e. ● In plurality voting, the winning candidate is the mode; the one with the most votes. Image Source Mode ● Continuous data are often discretized before computing the mode. ● The mode is intended as a measure of central tendency. ● But the mode need not be unique; some histograms are bi- or multi-modal. ● And the mode may not be representative of the bulk of the data. Image Source Mode Image Source Measures of Central Tendency ● Mean: numerical, preferably symmetric data without outliers ● Median: ordered, possibly skewed data, robust to outliers ● Mode: any data, but the only choice for categorical data ○ e.g., hair color, food items, movies, etc.


	y